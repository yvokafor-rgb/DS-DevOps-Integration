{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Br2AtMQ48IST"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline   # <-- imblearn pipeline (handles samplers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load file\n",
        "fraud = pd.read_csv('/content/drive/MyDrive/fraudTest.csv')\n",
        "\n",
        "print(fraud.head())\n",
        "print(fraud.shape)\n",
        "print(fraud.info())"
      ],
      "metadata": {
        "id": "4TfXdA0q8kaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cc24e64"
      },
      "source": [
        "# Task\n",
        "To prepare the `fraud` dataset for model training, first inspect the data for missing values and convert the `trans_date_trans_time` column to datetime objects. Then, drop the specified unnecessary columns: 'Unnamed: 0', 'cc_num', 'trans_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'unix_time', 'merch_lat', and 'merch_long'. Afterwards, identify the remaining categorical and numerical features. Apply `OneHotEncoder` to the categorical features and `StandardScaler` to the numerical features using a `ColumnTransformer`. Split the preprocessed data into training and testing sets, and finally, apply SMOTE to the training data to handle class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "975e30bf"
      },
      "source": [
        "## Inspect Data for Missing Values and Data Types\n",
        "\n",
        "### Subtask:\n",
        "Examine the 'fraud' DataFrame for missing values and review the data types of all columns to identify potential issues and determine appropriate preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45d25d7"
      },
      "source": [
        "**Reasoning**:\n",
        "To inspect for missing values and review data types, I will call the `.isnull().sum()` method on the `fraud` DataFrame to count missing values per column and then use the `.info()` method to display data types and non-null counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "149ce7ce",
        "outputId": "5d8d68e2-c9ef-4b08-856e-dfc0664b00da"
      },
      "source": [
        "print(\"Missing values per column:\")\n",
        "print(fraud.isnull().sum())\n",
        "\n",
        "print(\"\\nDataFrame Info (datatypes and non-null counts):\")\n",
        "fraud.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            "Unnamed: 0               0\n",
            "trans_date_trans_time    0\n",
            "cc_num                   0\n",
            "merchant                 0\n",
            "category                 0\n",
            "amt                      0\n",
            "first                    0\n",
            "last                     0\n",
            "gender                   0\n",
            "street                   0\n",
            "city                     0\n",
            "state                    0\n",
            "zip                      0\n",
            "lat                      0\n",
            "long                     0\n",
            "city_pop                 0\n",
            "job                      0\n",
            "dob                      0\n",
            "trans_num                0\n",
            "unix_time                0\n",
            "merch_lat                0\n",
            "merch_long               0\n",
            "is_fraud                 0\n",
            "dtype: int64\n",
            "\n",
            "DataFrame Info (datatypes and non-null counts):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 555719 entries, 0 to 555718\n",
            "Data columns (total 23 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Unnamed: 0             555719 non-null  int64  \n",
            " 1   trans_date_trans_time  555719 non-null  object \n",
            " 2   cc_num                 555719 non-null  int64  \n",
            " 3   merchant               555719 non-null  object \n",
            " 4   category               555719 non-null  object \n",
            " 5   amt                    555719 non-null  float64\n",
            " 6   first                  555719 non-null  object \n",
            " 7   last                   555719 non-null  object \n",
            " 8   gender                 555719 non-null  object \n",
            " 9   street                 555719 non-null  object \n",
            " 10  city                   555719 non-null  object \n",
            " 11  state                  555719 non-null  object \n",
            " 12  zip                    555719 non-null  int64  \n",
            " 13  lat                    555719 non-null  float64\n",
            " 14  long                   555719 non-null  float64\n",
            " 15  city_pop               555719 non-null  int64  \n",
            " 16  job                    555719 non-null  object \n",
            " 17  dob                    555719 non-null  object \n",
            " 18  trans_num              555719 non-null  object \n",
            " 19  unix_time              555719 non-null  int64  \n",
            " 20  merch_lat              555719 non-null  float64\n",
            " 21  merch_long             555719 non-null  float64\n",
            " 22  is_fraud               555719 non-null  int64  \n",
            "dtypes: float64(5), int64(6), object(12)\n",
            "memory usage: 97.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd1ba34"
      },
      "source": [
        "## Convert 'trans_date_trans_time' to Datetime\n",
        "\n",
        "### Subtask:\n",
        "Convert the 'trans_date_trans_time' column to a datetime object to enable time-based feature extraction if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c992aa7"
      },
      "source": [
        "**Reasoning**:\n",
        "To convert the 'trans_date_trans_time' column to datetime objects, I will use the `pd.to_datetime()` function and then verify the conversion by printing the column's data type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7045bc7e",
        "outputId": "b9dfeb25-66dc-4c13-a98a-2400000a3568"
      },
      "source": [
        "fraud['trans_date_trans_time'] = pd.to_datetime(fraud['trans_date_trans_time'])\n",
        "print(\"Data type of 'trans_date_trans_time' after conversion:\")\n",
        "print(fraud['trans_date_trans_time'].dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type of 'trans_date_trans_time' after conversion:\n",
            "datetime64[ns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19129b4b"
      },
      "source": [
        "## Drop Unnecessary Columns\n",
        "\n",
        "### Subtask:\n",
        "Drop the columns 'Unnamed: 0', 'cc_num', 'trans_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'unix_time', 'merch_lat', and 'merch_long' from the `fraud` DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03bbb57"
      },
      "source": [
        "**Reasoning**:\n",
        "To drop the specified columns, I will create a list of the columns to be dropped and then use the `df.drop()` method with `axis=1` to remove them from the `fraud` DataFrame. After dropping, I will display the first few rows of the DataFrame and its information to confirm the columns have been removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98734d9f"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the defined reasoning, I will now execute the code to drop the specified unnecessary columns from the `fraud` DataFrame. I will then verify the operation by displaying the first few rows and the DataFrame's info.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cfe32b",
        "outputId": "da32b9b6-4bdb-4a35-eb5d-ef026024fae7"
      },
      "source": [
        "columns_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'unix_time', 'merch_lat', 'merch_long']\n",
        "fraud = fraud.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "print(\"DataFrame after dropping columns (first 5 rows):\")\n",
        "print(fraud.head())\n",
        "print(\"\\nDataFrame Info after dropping columns:\")\n",
        "fraud.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after dropping columns (first 5 rows):\n",
            "  trans_date_trans_time                              merchant        category  \\\n",
            "0   2020-06-21 12:14:25                 fraud_Kirlin and Sons   personal_care   \n",
            "1   2020-06-21 12:14:33                  fraud_Sporer-Keebler   personal_care   \n",
            "2   2020-06-21 12:14:53  fraud_Swaniawski, Nitzsche and Welch  health_fitness   \n",
            "3   2020-06-21 12:15:15                     fraud_Haley Group        misc_pos   \n",
            "4   2020-06-21 12:15:17                 fraud_Johnston-Casper          travel   \n",
            "\n",
            "     amt gender  city_pop  is_fraud  \n",
            "0   2.86      M    333497         0  \n",
            "1  29.84      F       302         0  \n",
            "2  41.28      F     34496         0  \n",
            "3  60.05      M     54767         0  \n",
            "4   3.19      M      1126         0  \n",
            "\n",
            "DataFrame Info after dropping columns:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 555719 entries, 0 to 555718\n",
            "Data columns (total 7 columns):\n",
            " #   Column                 Non-Null Count   Dtype         \n",
            "---  ------                 --------------   -----         \n",
            " 0   trans_date_trans_time  555719 non-null  datetime64[ns]\n",
            " 1   merchant               555719 non-null  object        \n",
            " 2   category               555719 non-null  object        \n",
            " 3   amt                    555719 non-null  float64       \n",
            " 4   gender                 555719 non-null  object        \n",
            " 5   city_pop               555719 non-null  int64         \n",
            " 6   is_fraud               555719 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(1), int64(2), object(3)\n",
            "memory usage: 29.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29194e56"
      },
      "source": [
        "## Identify Categorical and Numerical Features\n",
        "\n",
        "### Subtask:\n",
        "Identify the remaining categorical and numerical features in the `fraud` DataFrame after dropping the specified columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "161676e9"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify categorical and numerical features, I will iterate through the DataFrame's columns and classify them based on their data types. Object data types will be considered categorical, while int64 and float64 will be considered numerical, excluding the target variable 'is_fraud'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9447eeb",
        "outputId": "60e0fa48-6d77-4b6a-885e-d9e30382363a"
      },
      "source": [
        "categorical_features = fraud.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = fraud.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Remove 'is_fraud' from numerical features if it's there, as it's the target variable\n",
        "if 'is_fraud' in numerical_features:\n",
        "    numerical_features.remove('is_fraud')\n",
        "\n",
        "# Remove 'trans_date_trans_time' from categorical if it somehow ended up there, or if it's not handled separately for feature engineering\n",
        "if 'trans_date_trans_time' in categorical_features:\n",
        "    categorical_features.remove('trans_date_trans_time')\n",
        "\n",
        "print(\"Categorical Features:\", categorical_features)\n",
        "print(\"Numerical Features:\", numerical_features)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Features: ['merchant', 'category', 'gender']\n",
            "Numerical Features: ['amt', 'city_pop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a844abe"
      },
      "source": [
        "## Apply Preprocessing to Features\n",
        "\n",
        "### Subtask:\n",
        "Apply `OneHotEncoder` to the categorical features and `StandardScaler` to the numerical features using a `ColumnTransformer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303bee82"
      },
      "source": [
        "## Apply Preprocessing to Features\n",
        "\n",
        "### Subtask:\n",
        "Apply `OneHotEncoder` to the categorical features and `StandardScaler` to the numerical features using a `ColumnTransformer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25dff6d6"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a `ColumnTransformer` by specifying the preprocessing steps for both categorical and numerical features. `OneHotEncoder` will be used for categorical features to convert them into a numerical format suitable for machine learning models, and `StandardScaler` will be applied to numerical features to standardize their scales, preventing features with larger values from dominating the model training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c5bcad"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply the defined preprocessing steps, I will create a `ColumnTransformer` with `OneHotEncoder` for the identified categorical features and `StandardScaler` for the numerical features. Then, I will fit and transform the relevant columns of the `fraud` DataFrame and display the shape of the resulting array to confirm the successful application of the transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ac12e55",
        "outputId": "f64c5316-3dc7-47b4-b394-4f778d4ea1e7"
      },
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X = fraud.drop('is_fraud', axis=1)\n",
        "\n",
        "# Drop 'trans_date_trans_time' from X as it's not meant for direct scaling or one-hot encoding\n",
        "if 'trans_date_trans_time' in X.columns:\n",
        "    X = X.drop('trans_date_trans_time', axis=1)\n",
        "\n",
        "y = fraud['is_fraud']\n",
        "\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"Shape of preprocessed data:\", X_preprocessed.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of preprocessed data: (555719, 711)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5639bfe4"
      },
      "source": [
        "## Split Data and Apply SMOTE\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed data into training and testing sets, and then apply SMOTE to the training data to handle class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56647cf1"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare the data for model training, I will first use `train_test_split` to divide the preprocessed features (`X_preprocessed`) and the target variable (`y`) into training and testing sets. This ensures that the model is evaluated on unseen data. Subsequently, I will apply `SMOTE` (Synthetic Minority Over-sampling Technique) to the *training data only* to address the potential class imbalance in the target variable (`is_fraud`), which is crucial for preventing bias towards the majority class during model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb733b85"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the reasoning, I will now split the preprocessed data into training and testing sets using `train_test_split` and then apply `SMOTE` to the training data to address class imbalance. I will display the shapes of the resulting datasets to confirm the operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f19b944",
        "outputId": "0ca902f3-221e-4f03-fe43-df651f85695a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Shape of X_train before SMOTE:\", X_train.shape)\n",
        "print(\"Shape of y_train before SMOTE:\", y_train.shape)\n",
        "print(\"Class distribution of y_train before SMOTE:\\n\", y_train.value_counts())\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nShape of X_train after SMOTE:\", X_train_res.shape)\n",
        "print(\"Shape of y_train after SMOTE:\", y_train_res.shape)\n",
        "print(\"Class distribution of y_train after SMOTE:\\n\", y_train_res.value_counts())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train before SMOTE: (444575, 711)\n",
            "Shape of y_train before SMOTE: (444575,)\n",
            "Class distribution of y_train before SMOTE:\n",
            " is_fraud\n",
            "0    442859\n",
            "1      1716\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Shape of X_train after SMOTE: (885718, 711)\n",
            "Shape of y_train after SMOTE: (885718,)\n",
            "Class distribution of y_train after SMOTE:\n",
            " is_fraud\n",
            "0    442859\n",
            "1    442859\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9fc89b5"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that all data preprocessing steps are completed and the data is ready for model training, including a summary of the transformations applied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "412f2830"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Missing Values and Data Types**: The initial inspection revealed no missing values across all 23 columns. The `trans_date_trans_time` column was identified as an `object` type requiring conversion to a datetime format.\n",
        "*   **Datetime Conversion**: The `trans_date_trans_time` column was successfully converted to the `datetime64[ns]` data type.\n",
        "*   **Column Removal**: A total of 16 unnecessary columns were successfully dropped, including 'Unnamed: 0', 'cc_num', 'trans_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'dob', 'unix_time', 'merch_lat', and 'merch_long', resulting in a DataFrame with 7 columns.\n",
        "*   **Feature Identification**: After dropping columns, the remaining features were classified:\n",
        "    *   Categorical features: `['merchant', 'category', 'gender']`\n",
        "    *   Numerical features: `['amt', 'city_pop']`\n",
        "*   **Preprocessing**:\n",
        "    *   A `ColumnTransformer` was used to apply `StandardScaler` to numerical features and `OneHotEncoder` to categorical features.\n",
        "    *   The `trans_date_trans_time` column was explicitly dropped from the feature set `X` before preprocessing to resolve a `ValueError` related to `datetime64[ns]` type not being supported by sparse matrices.\n",
        "    *   The final preprocessed feature matrix `X_preprocessed` had a shape of (555719, 711).\n",
        "*   **Data Splitting and Imbalance Handling**:\n",
        "    *   The preprocessed data was split into training and testing sets with a 80/20 ratio, stratified by the target variable `y`.\n",
        "    *   Before SMOTE, the training data exhibited significant class imbalance, with 442,859 instances of class 0 and only 1,716 instances of class 1.\n",
        "    *   SMOTE was applied to the training data, successfully balancing the classes to 442,859 instances for both class 0 and class 1. The resulting `X_train_res` and `y_train_res` had a shape of (885718, 711) and (885718,), respectively.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The data is now thoroughly preprocessed and ready for model training, with features scaled, categorical variables encoded, and class imbalance addressed in the training set.\n",
        "*   The next logical step is to train a machine learning model using the `X_train_res` and `y_train_res` datasets, followed by evaluating its performance on the unseen `X_test` and `y_test` datasets.\n"
      ]
    }
  ]
}